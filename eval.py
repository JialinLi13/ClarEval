import json
from collections import defaultdict
import sys
from datetime import datetime


class ResultSaver:
    def __init__(self, output_file="evaluation_results.txt"):
        self.output_file = output_file
        self.console_output = []

    def print_and_save(self, text):
        """åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°å’Œä¿å­˜åˆ°å†…å­˜"""
        print(text)
        self.console_output.append(text)

    def save_to_file(self):
        """å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        with open(self.output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"ä»£ç  Agent ä¸»åŠ¨æ¾„æ¸…èƒ½åŠ›è¯„ä¼°æŠ¥å‘Š\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {timestamp}\n")
            f.write("=" * 80 + "\n\n")

            for line in self.console_output:
                f.write(line + "\n")

        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°æ–‡ä»¶: {self.output_file}")


def calculate_average_metrics_by_ambiguity_type(file_path):
    # åˆå§‹åŒ–å­˜å‚¨ç»“æ„ - æŒ‰æ¨¡ç³Šç±»å‹åˆ†ç±»
    single_turn_metrics = {
        "missing_goal": defaultdict(list),
        "missing_premises": defaultdict(list),
        "ambiguous_terms": defaultdict(list)
    }

    multi_turn_metrics = {
        "missing_goal": defaultdict(list),
        "missing_premises": defaultdict(list),
        "ambiguous_terms": defaultdict(list)
    }

    # è¯»å–æ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data = json.loads(line.strip())
                task_id = data.get("task_id", "")
                scenario = data.get("scenario", "")
                metrics = data.get("metrics", {})

                # ç¡®å®šæ¨¡ç³Šç±»å‹
                ambiguity_type = None
                if "missing_goal" in task_id:
                    ambiguity_type = "missing_goal"
                elif "missing_premises" in task_id:
                    ambiguity_type = "missing_premises"
                elif "ambiguous_terms" in task_id:
                    ambiguity_type = "ambiguous_terms"

                if not ambiguity_type:
                    continue

                if "Single-Turn" in scenario:
                    # å•è½®åœºæ™¯æŒ‡æ ‡
                    target_dict = single_turn_metrics[ambiguity_type]
                    target_dict["kqc_coverage"].append(metrics.get("kqc_coverage", 0))
                    target_dict["mpr_coverage"].append(metrics.get("mpr_coverage", 0))
                    target_dict["total_gold_questions"].append(metrics.get("total_gold_questions", 0))
                    target_dict["total_gold_premises"].append(metrics.get("total_gold_premises", 0))
                    target_dict["matched_kqc_count"].append(metrics.get("matched_kqc_count", 0))
                    target_dict["matched_mpr_count"].append(metrics.get("matched_mpr_count", 0))
                    target_dict["unmatched_kqc_count"].append(metrics.get("unmatched_kqc_count", 0))
                    target_dict["unmatched_mpr_count"].append(metrics.get("unmatched_mpr_count", 0))

                elif "Multi-Turn" in scenario:
                    # å¤šè½®åœºæ™¯æŒ‡æ ‡
                    target_dict = multi_turn_metrics[ambiguity_type]
                    target_dict["clarification_coverage"].append(metrics.get("clarification_coverage", 0))
                    target_dict["efficiency_ratio"].append(metrics.get("efficiency_ratio", 0))
                    target_dict["atc"].append(metrics.get("atc", 0))
                    target_dict["total_required_clarifications"].append(metrics.get("total_required_clarifications", 0))
                    target_dict["agent_question_turns"].append(metrics.get("agent_question_turns", 0))
                    target_dict["clarified_premises_count"].append(metrics.get("clarified_premises_count", 0))
                    target_dict["unmatched_clarifications_count"].append(
                        metrics.get("unmatched_clarifications_count", 0))

    # è®¡ç®—æ¯ç§æ¨¡ç³Šç±»å‹çš„å¹³å‡å€¼
    def calculate_type_averages(metrics_dict):
        results = {}
        for amb_type, metrics in metrics_dict.items():
            type_avg = {}
            for key, values in metrics.items():
                if values:
                    type_avg[f"avg_{key}"] = sum(values) / len(values)
                    type_avg[f"total_{key}"] = sum(values)
                    type_avg[f"count_{key}"] = len(values)
            results[amb_type] = type_avg
        return results

    single_turn_results = calculate_type_averages(single_turn_metrics)
    multi_turn_results = calculate_type_averages(multi_turn_metrics)

    return {
        "single_turn": single_turn_results,
        "multi_turn": multi_turn_results
    }


def print_detailed_results_by_type(results, saver):
    """æŒ‰æ¨¡ç³Šç±»å‹æ‰“å°è¯¦ç»†çš„ç»Ÿè®¡ç»“æœ"""

    saver.print_and_save("=" * 80)
    saver.print_and_save("ä»£ç  Agent ä¸»åŠ¨æ¾„æ¸…èƒ½åŠ›è¯„ä¼° - æŒ‰æ¨¡ç³Šç±»å‹åˆ†ç±»ç»Ÿè®¡")
    saver.print_and_save("=" * 80)

    # å•è½®åœºæ™¯ç»“æœ
    saver.print_and_save("\nğŸ“Š å•è½®åœºæ™¯ (Single-Turn) - æŒ‰æ¨¡ç³Šç±»å‹åˆ†ç±»:")
    saver.print_and_save("=" * 60)

    single_turn = results["single_turn"]

    for amb_type in ["missing_goal", "missing_premises", "ambiguous_terms"]:
        data = single_turn.get(amb_type, {})
        if not data:
            continue

        saver.print_and_save(f"\nğŸ” {amb_type.replace('_', ' ').title()}:")
        saver.print_and_save("-" * 40)
        saver.print_and_save(f"   ğŸ”‘ å…³é”®é—®é¢˜è¦†ç›–ç‡ (KQC): {data.get('avg_kqc_coverage', 0):.4f}")
        saver.print_and_save(f"   ğŸ“‹ ç¼ºå¤±å‰æå¬å›ç‡ (MPR): {data.get('avg_mpr_coverage', 0):.4f}")
        saver.print_and_save(
            f"   ğŸ“ˆ å¹³å‡åŒ¹é…KQCæ•°é‡: {data.get('avg_matched_kqc_count', 0):.2f} / {data.get('avg_total_gold_questions', 0):.2f}")
        saver.print_and_save(
            f"   ğŸ“ˆ å¹³å‡åŒ¹é…MPRæ•°é‡: {data.get('avg_matched_mpr_count', 0):.2f} / {data.get('avg_total_gold_premises', 0):.2f}")
        saver.print_and_save(f"   âŒ å¹³å‡æœªåŒ¹é…KQC: {data.get('avg_unmatched_kqc_count', 0):.2f}")
        saver.print_and_save(f"   âŒ å¹³å‡æœªåŒ¹é…MPR: {data.get('avg_unmatched_mpr_count', 0):.2f}")
        saver.print_and_save(f"   ğŸ“Š ä»»åŠ¡æ•°é‡: {data.get('count_kqc_coverage', 0)}")

    # å¤šè½®åœºæ™¯ç»“æœ
    saver.print_and_save("\nğŸ”„ å¤šè½®åœºæ™¯ (Multi-Turn) - æŒ‰æ¨¡ç³Šç±»å‹åˆ†ç±»:")
    saver.print_and_save("=" * 60)

    multi_turn = results["multi_turn"]

    for amb_type in ["missing_goal", "missing_premises", "ambiguous_terms"]:
        data = multi_turn.get(amb_type, {})
        if not data:
            continue

        saver.print_and_save(f"\nğŸ” {amb_type.replace('_', ' ').title()}:")
        saver.print_and_save("-" * 40)
        saver.print_and_save(f"   ğŸ¯ æ¾„æ¸…è¦†ç›–ç‡: {data.get('avg_clarification_coverage', 0):.4f}")
        saver.print_and_save(f"   âš¡ æ•ˆç‡æ¯”ç‡: {data.get('avg_efficiency_ratio', 0):.4f}")
        saver.print_and_save(f"   â±ï¸  å¹³å‡æ¾„æ¸…è½®æ•° (ATC): {data.get('avg_atc', 0):.4f}")
        saver.print_and_save(
            f"   ğŸ“Š å¹³å‡æ¾„æ¸…å‰ææ•°é‡: {data.get('avg_clarified_premises_count', 0):.2f} / {data.get('avg_total_required_clarifications', 0):.2f}")
        saver.print_and_save(f"   ğŸ”„ å¹³å‡æé—®è½®æ¬¡: {data.get('avg_agent_question_turns', 0):.2f}")
        saver.print_and_save(f"   âŒ å¹³å‡æœªåŒ¹é…æ¾„æ¸…: {data.get('avg_unmatched_clarifications_count', 0):.2f}")
        saver.print_and_save(f"   ğŸ“Š ä»»åŠ¡æ•°é‡: {data.get('count_clarification_coverage', 0)}")


def calculate_overall_averages(results):
    """è®¡ç®—æ€»ä½“å¹³å‡å€¼ï¼ˆä¸åˆ†ç±»å‹ï¼‰"""
    single_turn_overall = defaultdict(list)
    multi_turn_overall = defaultdict(list)

    # å•è½®åœºæ™¯æ€»ä½“å¹³å‡
    for amb_type, metrics in results["single_turn"].items():
        for key, value in metrics.items():
            if key.startswith("avg_"):
                metric_name = key.replace("avg_", "")
                single_turn_overall[metric_name].append(value)

    # å¤šè½®åœºæ™¯æ€»ä½“å¹³å‡
    for amb_type, metrics in results["multi_turn"].items():
        for key, value in metrics.items():
            if key.startswith("avg_"):
                metric_name = key.replace("avg_", "")
                multi_turn_overall[metric_name].append(value)

    # è®¡ç®—æ€»ä½“å¹³å‡
    single_avg = {f"overall_avg_{k}": sum(v) / len(v) if v else 0 for k, v in single_turn_overall.items()}
    multi_avg = {f"overall_avg_{k}": sum(v) / len(v) if v else 0 for k, v in multi_turn_overall.items()}

    return {
        "single_turn": single_avg,
        "multi_turn": multi_avg
    }


def print_comparison_table(results, saver):
    """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""
    saver.print_and_save("\n" + "=" * 80)
    saver.print_and_save("ğŸ“‹ æ¨¡ç³Šç±»å‹å¯¹æ¯”è¡¨æ ¼")
    saver.print_and_save("=" * 80)

    single_turn = results["single_turn"]
    multi_turn = results["multi_turn"]

    # å•è½®åœºæ™¯å¯¹æ¯”
    saver.print_and_save("\nğŸ“Š å•è½®åœºæ™¯æŒ‡æ ‡å¯¹æ¯”:")
    saver.print_and_save("-" * 70)
    saver.print_and_save(f"{'æ¨¡ç³Šç±»å‹':<20} {'KQC':<8} {'MPR':<8} {'ä»»åŠ¡æ•°é‡':<10}")
    saver.print_and_save("-" * 70)

    for amb_type in ["missing_goal", "missing_premises", "ambiguous_terms"]:
        data = single_turn.get(amb_type, {})
        if data:
            kqc = data.get('avg_kqc_coverage', 0)
            mpr = data.get('avg_mpr_coverage', 0)
            count = data.get('count_kqc_coverage', 0)
            saver.print_and_save(f"{amb_type.replace('_', ' ').title():<20} {kqc:<8.3f} {mpr:<8.3f} {count:<10}")

    # å¤šè½®åœºæ™¯å¯¹æ¯”
    saver.print_and_save("\nğŸ”„ å¤šè½®åœºæ™¯æŒ‡æ ‡å¯¹æ¯”:")
    saver.print_and_save("-" * 70)
    saver.print_and_save(f"{'æ¨¡ç³Šç±»å‹':<20} {'æ¾„æ¸…è¦†ç›–ç‡':<10} {'æ•ˆç‡æ¯”ç‡':<10} {'ATC':<8} {'ä»»åŠ¡æ•°é‡':<10}")
    saver.print_and_save("-" * 70)

    for amb_type in ["missing_goal", "missing_premises", "ambiguous_terms"]:
        data = multi_turn.get(amb_type, {})
        if data:
            coverage = data.get('avg_clarification_coverage', 0)
            efficiency = data.get('avg_efficiency_ratio', 0)
            atc = data.get('avg_atc', 0)
            count = data.get('count_clarification_coverage', 0)
            saver.print_and_save(
                f"{amb_type.replace('_', ' ').title():<20} {coverage:<10.3f} {efficiency:<10.3f} {atc:<8.3f} {count:<10}")


def print_paper_format(results, overall_results, saver):
    """è¾“å‡ºè®ºæ–‡æ ¼å¼çš„ç»“æœ"""
    saver.print_and_save("\n" + "=" * 80)
    saver.print_and_save("ğŸ“„ è®ºæ–‡ç»“æœæ ¼å¼")
    saver.print_and_save("=" * 80)

    single_turn = results["single_turn"]
    multi_turn = results["multi_turn"]

    saver.print_and_save("\nå•è½®åœºæ™¯ç»“æœ:")
    saver.print_and_save("-" * 40)
    for amb_type in ["missing_goal", "missing_premises", "ambiguous_terms"]:
        data = single_turn.get(amb_type, {})
        if data:
            kqc = data.get('avg_kqc_coverage', 0)
            mpr = data.get('avg_mpr_coverage', 0)
            saver.print_and_save(f"{amb_type.replace('_', ' ').title()}: KQC={kqc:.3f}, MPR={mpr:.3f}")

    saver.print_and_save(
        f"\nå•è½®åœºæ™¯æ€»ä½“å¹³å‡: KQC={overall_results['single_turn'].get('overall_avg_kqc_coverage', 0):.3f}, MPR={overall_results['single_turn'].get('overall_avg_mpr_coverage', 0):.3f}")

    saver.print_and_save("\nå¤šè½®åœºæ™¯ç»“æœ:")
    saver.print_and_save("-" * 40)
    for amb_type in ["missing_goal", "missing_premises", "ambiguous_terms"]:
        data = multi_turn.get(amb_type, {})
        if data:
            coverage = data.get('avg_clarification_coverage', 0)
            efficiency = data.get('avg_efficiency_ratio', 0)
            atc = data.get('avg_atc', 0)
            saver.print_and_save(
                f"{amb_type.replace('_', ' ').title()}: æ¾„æ¸…è¦†ç›–ç‡={coverage:.3f}, æ•ˆç‡æ¯”ç‡={efficiency:.3f}, ATC={atc:.3f}")

    saver.print_and_save(
        f"\nå¤šè½®åœºæ™¯æ€»ä½“å¹³å‡: æ¾„æ¸…è¦†ç›–ç‡={overall_results['multi_turn'].get('overall_avg_clarification_coverage', 0):.3f}, æ•ˆç‡æ¯”ç‡={overall_results['multi_turn'].get('overall_avg_efficiency_ratio', 0):.3f}, ATC={overall_results['multi_turn'].get('overall_avg_atc', 0):.3f}")


def save_structured_data(results, overall_results, filename="structured_results.json"):
    """ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°JSONæ–‡ä»¶"""
    structured_data = {
        "detailed_results": results,
        "overall_results": overall_results,
        "analysis_timestamp": datetime.now().isoformat()
    }

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(structured_data, f, indent=2, ensure_ascii=False)

    print(f"âœ… ç»“æ„åŒ–æ•°æ®å·²ä¿å­˜åˆ°: {filename}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    input_file = "gpt5_clareval_results.jsonl"  # æ›¿æ¢ä¸ºæ‚¨çš„è¾“å…¥æ–‡ä»¶è·¯å¾„
    output_file = "gpt5_clareval_report.jsonl"  # è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶
    json_file = "evaluation_data.jsonl"  # ç»“æ„åŒ–æ•°æ®æ–‡ä»¶

    # åˆå§‹åŒ–ç»“æœä¿å­˜å™¨
    saver = ResultSaver(output_file)

    try:
        # è®¡ç®—æŒ‰æ¨¡ç³Šç±»å‹åˆ†ç±»çš„æŒ‡æ ‡
        detailed_results = calculate_average_metrics_by_ambiguity_type(input_file)

        # æ‰“å°è¯¦ç»†ç»“æœ
        print_detailed_results_by_type(detailed_results, saver)

        # æ‰“å°å¯¹æ¯”è¡¨æ ¼
        print_comparison_table(detailed_results, saver)

        # è®¡ç®—æ€»ä½“å¹³å‡
        overall_results = calculate_overall_averages(detailed_results)

        # è¾“å‡ºè®ºæ–‡æ ¼å¼
        print_paper_format(detailed_results, overall_results, saver)

        # é¢å¤–ç»Ÿè®¡ï¼šå„ç±»å‹çš„ä»»åŠ¡åˆ†å¸ƒ
        saver.print_and_save("\n" + "=" * 80)
        saver.print_and_save("ğŸ“ˆ ä»»åŠ¡åˆ†å¸ƒç»Ÿè®¡")
        saver.print_and_save("=" * 80)

        single_count = {amb_type: data.get('count_kqc_coverage', 0) for amb_type, data in
                        detailed_results["single_turn"].items()}
        multi_count = {amb_type: data.get('count_clarification_coverage', 0) for amb_type, data in
                       detailed_results["multi_turn"].items()}

        saver.print_and_save(f"å•è½®åœºæ™¯ä»»åŠ¡åˆ†å¸ƒ: {single_count}")
        saver.print_and_save(f"å¤šè½®åœºæ™¯ä»»åŠ¡åˆ†å¸ƒ: {multi_count}")
        saver.print_and_save(f"å•è½®åœºæ™¯æ€»ä»»åŠ¡æ•°: {sum(single_count.values())}")
        saver.print_and_save(f"å¤šè½®åœºæ™¯æ€»ä»»åŠ¡æ•°: {sum(multi_count.values())}")

        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        saver.save_to_file()

        # ä¿å­˜ç»“æ„åŒ–æ•°æ®åˆ°JSON
        save_structured_data(detailed_results, overall_results, json_file)

        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {output_file}")
        print(f"ğŸ“Š ç»“æ„åŒ–æ•°æ®: {json_file}")

    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ '{input_file}'")
        print("è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œç„¶åé‡æ–°è¿è¡Œç¨‹åºã€‚")
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback

        traceback.print_exc()